{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:**\\\n",
    "Samuel Reyes Sanz\\\n",
    "Eduardo Miguel Riederer\n",
    "\n",
    "# Multi-Author Writing Style Analysis 2023\n",
    "\n",
    "The goal of the style change detection task is to identify text positions within a given multi-author document at which the author switches.\n",
    "\n",
    "The simultaneous change of authorship and topic will be carefully controlled and we will provide participants with datasets of three difficulty levels:\n",
    "\n",
    "- Easy: The paragraphs of a document cover a variety of topics, allowing approaches to make use of topic information to detect authorship changes.\n",
    "- Medium: The topical variety in a document is small (though still present) forcing the approaches to focus more on style to effectively solve the detection task.\n",
    "- Hard: All paragraphs in a document are on the same topic.\n",
    "\n",
    "All documents are provided in English and may contain an arbitrary number of style changes. However, style changes may only occur between paragraphs (i.e., a single paragraph is always authored by a single author and contains no style changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samu/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/samu/.conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Edu\n",
    "#PATH = 'C:\\\\Users\\\\nobody\\\\Downloads\\\\Master\\\\NLP\\\\pan23-multi-author-analysis'\n",
    "#TRAIN_PATH = os.path.join(PATH, 'data\\\\pan23-multi-author-analysis-dataset1\\\\pan23-multi-author-analysis-dataset1-train')\n",
    "#TEST_PATH = os.path.join(PATH, 'data\\\\pan23-multi-author-analysis-dataset1\\\\pan23-multi-author-analysis-dataset1-validation')\n",
    "\n",
    "# Samu\n",
    "PATH = '/home/samu/workspace/projects/UPM/NLP/pan23-multi-author-analysis'\n",
    "TRAIN_PATH = os.path.join(PATH, 'data/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-train')\n",
    "TEST_PATH = os.path.join(PATH, 'data/pan23-multi-author-analysis-dataset3/pan23-multi-author-analysis-dataset3-validation')\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "VAL_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 512\n",
    "TOKENIZER = DebertaV2TokenizerFast.from_pretrained(\"microsoft/deberta-v3-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Se separan los párrafos con el token [SEP] y se colocan por pares en un DataFrame junto a su etiqueta: \n",
    "- 0: no ha cambiado el autor\n",
    "- 1: ha cambiado el autor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si un texto tiene 4 párrafos (ABCD) se dividirá en 3 instancias: AB, BC, CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    text_files = [f for f in os.listdir(folder_path) if f.startswith('problem') and f.endswith('.txt')]\n",
    "    rows = []\n",
    "\n",
    "    for text_file in text_files:\n",
    "        text_file_path = os.path.join(folder_path, text_file)\n",
    "        label_file_path = text_file_path.replace('.txt', '.json').replace('problem', 'truth-problem')\n",
    "\n",
    "        with open(text_file_path, 'r', encoding=\"utf8\") as file:\n",
    "            paragraphs = file.read().split('\\n')\n",
    "\n",
    "        with open(label_file_path, 'r', encoding=\"utf8\") as file:\n",
    "            labels = json.load(file)['changes']\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            combined_text = paragraphs[i] + ' [SEP] ' + paragraphs[i+1] \n",
    "            label = labels[i] \n",
    "            rows.append({'Text': combined_text, 'Label': label})\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and test data\n",
    "train_df = load_data(TRAIN_PATH)\n",
    "test_df = load_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    10092\n",
       "1     9021\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del conjunto de datos\n",
    "\n",
    "Se detecta que existen carácteres especiales como emojis o letras chinas. Deberta usa sentencepiece tokenizer que soporta diferentes idiomas y carácteres especiales como emojis (https://github.com/google/sentencepiece)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16612\n",
      "16613\n"
     ]
    }
   ],
   "source": [
    "def find_special_characters(df):\n",
    "    special_chars = set()\n",
    "    texts_withs_special_characters = []\n",
    "    \n",
    "    for index, text in enumerate(df['Text']):\n",
    "        for char in text:\n",
    "            if not char.isascii():\n",
    "                special_chars.add(char)\n",
    "                texts_withs_special_characters.append(index)\n",
    "                \n",
    "                if char == \"🥴\":\n",
    "                    print (index)\n",
    "                    \n",
    "    return (list(special_chars), texts_withs_special_characters)\n",
    "\n",
    "special_chars, texts_withs_special_characters = find_special_characters(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['⚡', '遼', '\\u202f', '茶', '…', 'ú', 'П', 'ı', '’', 'ż', 'ṣ', '国', 'ç', '️', '天', '́', '夏', '—', '·', 'ã', 'κ', 'ḗ', '愈', 'í', '🌍', 'ł', '🤍', '😂', 'â', 'è', 'ʿ', '🗳', '🙂', '客', '取', 'К', 'н', '劉', 'ȝ', '©', 'ü', '規', '≠', 'э', '馬', 'Χ', '₂', 'з', 'Н', '€', '🤯', 'с', '\\u2003', '\\u200b', 'ι', 'ġ', '🏡', 'Γ', '宗', 'τ', '成', 'п', 'á', '¢', 'ē', 'Ç', '⚖', '銃', '覺', '“', '\\xad', '愛', '⅓', 'ś', '\\u2002', 'ş', '篇', '店', '¹', '«', '£', '§', '棣', 'ū', 'ō', '•', '🤙', 'ṇ', 'и', 'ä', 'Ε', 'ī', '號', 'Ō', '登', 'ʰ', 'ó', '貞', '邦', 'đ', 'ð', '性', '🥴', '秋', '🙃', 'к', '國', '清', '慧', 'ο', '翱', 'Ü', '年', 'ῑ', '中', 'ğ', '封', '🏼', '‘', '₁', 'т', '可', 'ц', '😑', '🤣', '祖', '卑', '„', 'ˈ', '周', 'е', '名', '永', '×', 'ö', '喫', 'σ', '總', '基', 'č', '唐', '🤬', '–', 'л', '西', '❗', '”', '民', '민', '🙏', 'ы', 'ô', '‡', '🗽', '司', '⅔', 'Ř', '😷', 'а', 'î', 'ك', '¿', '😔', '♂', '涨', 'ч', 'ʒ', '\\U0001fae2', '高', '🏻', '韓', 'ñ', '\\u2060', 'ή', '🇸', '義', 'Ṛ', 'þ', '🇨', '⬇', 'ý', '🤔', 'å', 'о', '족', 'ɔ', 'ø', '樂', '資', '朱', '̂', '☠', 'Š', 'û', 'ò', '觀', '̯', '🙄', '則', 'ω', 'в', 'х', 'ά', '建', 'ā', 'ν', '燁', '皇', 'м', 'ς', 'Í', 'α', '🇺', '元', '迷', 'υ', 'æ', 'В', '錄', '重', '帝', 'š', 'ɑ', 'é', 'Á', 'ə', '»', '商', '主', '🥵', '屋', '漢', '始', '上', 'г', '\\u200d', '東', '►', '政', 'ḫ', 'ː', 'ρ', '魏', '祥', '吳', 'ό', '王', '乐', '發', 'р', '李', '太', 'À', 'ε', 'Ş', '😅', 'ê', 'Ö', 'ш', '°', '明', 'б', '春', '\\xa0', '世', 'ί', '™', '羅', '估', 'Â', '🤪', 'λ', '新', '締', '🤷', '秦', 'я', 'Ḫ', '玄', 'º', 'ю', '你', '🏽', '🥺', '地', 'ë', 'ь', '姓', '統', '🇦', 'à', 'ß', '遷', '⁰', '´', '金', '戰', '砲', '大', 'д', 'ǔ', 'у', '🇭', 'İ', 'Ś', '🗨']\n"
     ]
    }
   ],
   "source": [
    "print(special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5612"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numero de textos con caracteres especiales\n",
    "\n",
    "texts_withs_special_characters = sorted(set(texts_withs_special_characters))\n",
    "texts_withs_special_characters\n",
    "len(texts_withs_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tokeniza el texto del emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/samu/.conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 507, 125322, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization = TOKENIZER.encode_plus(\n",
    "            text = \"😐\",\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "            )\n",
    "\n",
    "tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de truncación\n",
    "\n",
    "Dado que el tamaño máximo de entrada del modelo DeBERTa-v3 es 512 tokens, comprobamos el número de textos con más de 500 palabras. Para los pocos casos en los que se supere el tamño de entrada, utilizaremos **Transition-Focused Truncation** para recortar las entradas. En las entradas que sean menores a 512 tokens, se aplicará padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(text.split()) > 512 for text in train_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_focused_truncation(text, tokenizer, max_tokens = MAX_LEN, sep_token=\"[SEP]\"):\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # CASE WHERE TOKENS DO NOT EXCEED LIMIT\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    # CASE WHERE TOKENS EXCEED LIMIT\n",
    "   \n",
    "    # Find the index of the [SEP] token\n",
    "    sep_index = tokens.index(sep_token) if sep_token in tokens else -1\n",
    "    \n",
    "    # If [SEP] is not found, truncate the first max_tokens\n",
    "    if sep_index == -1:\n",
    "        return tokens[:max_tokens]\n",
    "\n",
    "    # Let's check what text is longer\n",
    "    text_1 = tokens[:sep_index]\n",
    "    text_2 = tokens[(sep_index + 1):]\n",
    "    \n",
    "    if (len(text_1) >= max_tokens // 2):\n",
    "        \n",
    "        #Case where both texts are bigger than max_tokens / 2\n",
    "        if (len(text_2) >= max_tokens // 2):\n",
    "            text_1 = text_1[int(len(text_1)-(max_tokens//2)):]\n",
    "            \n",
    "            if int(len(text_2) - (max_tokens//2)) != 0:\n",
    "                text_2 = text_2[:-int(len(text_2) - (max_tokens//2))]\n",
    "            else:\n",
    "                text_2 = text_2[:]\n",
    "                \n",
    "            if max_tokens % 2 == 0:\n",
    "                rand_num = random.randint(0, 1)\n",
    "                if rand_num == 0:\n",
    "                    text_1 = text_1[1:]\n",
    "                else:\n",
    "                    text_2 = text_2[:-1]\n",
    "                \n",
    "        #Case where only text_1 is bigger than max_tokens / 2\n",
    "        else:\n",
    "            max_tokens_for_text = max_tokens - len(text_2)\n",
    "            text_1 = text_1[-max_tokens_for_text:]\n",
    "            \n",
    "            if (len(text_1) + len(text_2) == max_tokens):\n",
    "                text_1 = text_1[1:]\n",
    "                \n",
    "    #Case where only text_2 is bigger than max_tokens / 2\n",
    "    else:\n",
    "        max_tokens_for_text = max_tokens - len(text_1)\n",
    "        text_2 = text_2[:max_tokens_for_text]\n",
    "        \n",
    "        if (len(text_1) + len(text_2) == max_tokens):\n",
    "            text_2 = text_2[:-1]\n",
    "        \n",
    "    return (text_1 + [sep_token] + text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When Russia says Nazi they are not engaging in doublespeak, where they accuse the enemy of doing exactly what they do, themselves. To them \"Nazi\" simply means \"western enemy of russia.\" Russians don\\'t care about torture or concentration camps, why would they? It\\'s their preferred solution for political differences. Even russian mercenaries ride into battle with nazi symbology tattooed on them.[SEP] I don\\'t know if I was clear. I am saying people perceive russia is engaging in nazi tactics while claiming they hate nazis, thus engaging in doublespeak. But this is not how russia sees it because the nazis tactics were not the defining quality of what made them nazi'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "long_text = train_df[\"Text\"][781]\n",
    "tokens = transition_focused_truncation(long_text, TOKENIZER)\n",
    "\"\".join(tokens).replace(\"▁\", \" \")[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Se crea el dataset personalizado en el que se devuelve una instancia de texto tokenizado, junto a la máscara, los token type ids y las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len=MAX_LEN):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.Text\n",
    "        self.targets = self.data.Label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        \n",
    "        truncated_text = transition_focused_truncation(text, self.tokenizer, self.max_len, \"[SEP]\")\n",
    "\n",
    "        batch_encoder = self.tokenizer(\n",
    "            truncated_text,\n",
    "            is_split_into_words=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        ids = batch_encoder['input_ids']\n",
    "        mask = batch_encoder['attention_mask']\n",
    "        token_type_ids = batch_encoder[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': ids.squeeze(0),\n",
    "            'mask': mask.squeeze(0),\n",
    "            'token_type_ids': token_type_ids.squeeze(0),\n",
    "            'targets':self.targets[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: 19113\n",
      "TEST Dataset: 4112\n"
     ]
    }
   ],
   "source": [
    "training_dataset = CustomDataset(dataframe=train_df, tokenizer=TOKENIZER, max_len=MAX_LEN)\n",
    "testing_dataset = CustomDataset(dataframe=test_df, tokenizer=TOKENIZER, max_len=MAX_LEN)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True)\n",
    "testing_loader = DataLoader(testing_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"TRAIN Dataset: {}\".format(len(training_dataset)))\n",
    "print(\"TEST Dataset: {}\".format(len(testing_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model to fine-tune\n",
    "\n",
    "Se añade la cabeza de clasificación. Se ha utilizado el modelo DebertaV3 (https://arxiv.org/abs/2111.09543), una versión mejorada del modelo Deberta gracias a entrenar el modelo utilizando replaced token detection (RTD), una técnica más sofisticada que emplear masked language modeling (MLM). Este modelo admite como entrada un tamaño de secuencia de 512 y devuelve un tamaño de embedding de 768. Se probó a emplear la versión large de Deberta V3, pero se descartó su uso ya que proporciona resultados muy similares, pero aumenta en gran medida el tiempo de aprendizaje y los requisitos de memoria GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la salida del modelo se añaden las siguientes capas:\n",
    "- Inicialmente se realiza la media de todos los embeddings de la secuencia, generando un embedding que representa a los dos párrafos.\n",
    "- Utilizando este embedding, se procesa a través de una cabeza de clasificación compuesta por capas densas, de normalización, de dropout y funciones de activación GELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 18:22:42.998932: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 18:22:43.016662: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 18:22:43.016677: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 18:22:43.016693: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 18:22:43.021217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 18:22:43.397068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeBERTaClass(\n",
       "  (l1): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pool): MeanPooling()\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distilBERT to get the final \n",
    "# output for the model.\n",
    "\n",
    "class DeBERTaClass(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DeBERTaClass, self).__init__()\n",
    "        self.dropout = 0.2\n",
    "        self.hidden_embd = 768\n",
    "        self.output_layer = 1\n",
    "        \n",
    "        self.l1 = AutoModel.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "        self.pool = MeanPooling()\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_embd, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.Linear(128, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.Linear(32, self.output_layer)\n",
    "        )\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False) #bs, sl, es\n",
    "        last_hidden_states = output_1[0]\n",
    "        feature = self.pool(last_hidden_states, mask)\n",
    "        output = self.head(feature)\n",
    "\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "model = DeBERTaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida utilizada será la entropía cruzada binaria, usando la versión \"with logits\" para evitar tener que aplicar una capa con la función sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.functional.binary_cross_entropy_with_logits(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El optimizador empleado es Adam, uno de los más populares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con epochs fijas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float) # bs,\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, targets) # bs, sl, 1 / bs\n",
    "\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(EPOCHS):\\n    train(epoch)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = None\n",
    "        self.epochs_no_improve = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_metric):\n",
    "        if self.best_metric == None:\n",
    "            self.best_metric = val_metric\n",
    "        elif self.best_metric - val_metric > self.min_delta:\n",
    "            self.best_metric = val_metric\n",
    "            self.epochs_no_improve = 0\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                self.should_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validation_loader, model, loss_fn, device):\n",
    "    \n",
    "    # Poner el modelo en modo de evaluación\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Desactivar el cálculo del gradiente\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(validation_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)  # bs,\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Suponiendo una tarea de clasificación, convertir las salidas a predicciones\n",
    "            predictions = torch.round(torch.sigmoid(outputs)).squeeze()  # Convertir logits a probabilidades y luego a clases binarias\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(validation_loader)\n",
    "    accuracy = (np.array(all_predictions) == np.array(all_targets)).mean()\n",
    "    f1 = f1_score(all_targets, all_predictions)\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienza el entrenamiento. Se guarda el modelo con mayor F1 en validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7505934834480286\n",
      "Epoch: 0,\n",
      "Validación - Avg Loss: 0.6710, Accuracy: 0.5501, F1 Score: 0.6689\n",
      "Epoch: 1, Loss:  0.7667965292930603\n",
      "Epoch: 1,\n",
      "Validación - Avg Loss: 0.4718, Accuracy: 0.7636, F1 Score: 0.7416\n",
      "Epoch: 2, Loss:  0.5437997579574585\n",
      "Epoch: 2,\n",
      "Validación - Avg Loss: 0.4262, Accuracy: 0.7938, F1 Score: 0.7818\n",
      "Epoch: 3, Loss:  0.32313966751098633\n",
      "Epoch: 3,\n",
      "Validación - Avg Loss: 0.4258, Accuracy: 0.7979, F1 Score: 0.7802\n",
      "Epoch: 4, Loss:  0.194144606590271\n",
      "Epoch: 4,\n",
      "Validación - Avg Loss: 0.4929, Accuracy: 0.7977, F1 Score: 0.8055\n",
      "Epoch: 5, Loss:  0.11288212239742279\n",
      "Epoch: 5,\n",
      "Validación - Avg Loss: 0.4771, Accuracy: 0.8067, F1 Score: 0.8111\n",
      "Epoch: 6, Loss:  0.10796435922384262\n",
      "Epoch: 6,\n",
      "Validación - Avg Loss: 0.5213, Accuracy: 0.8011, F1 Score: 0.7993\n",
      "Epoch: 7, Loss:  0.13444465398788452\n",
      "Epoch: 7,\n",
      "Validación - Avg Loss: 0.5804, Accuracy: 0.7921, F1 Score: 0.7767\n",
      "Epoch: 8, Loss:  0.09689974039793015\n",
      "Epoch: 8,\n",
      "Validación - Avg Loss: 0.6358, Accuracy: 0.7967, F1 Score: 0.7901\n",
      "Epoch: 9, Loss:  0.04825149476528168\n",
      "Epoch: 9,\n",
      "Validación - Avg Loss: 0.6624, Accuracy: 0.7945, F1 Score: 0.7848\n"
     ]
    }
   ],
   "source": [
    "best_f1_score = 0.0\n",
    "best_model_state = None\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    avg_loss, accuracy, f1 = validate(testing_loader, model, loss_fn, device)\n",
    "    print(f'Epoch: {epoch},\\nValidación - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        torch.save(model, 'modelo3-base-task3.pth')\n",
    "\n",
    "    early_stopping(f1)\n",
    "\n",
    "    if early_stopping.should_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEJOR F1 EN VALIDACIÓN 81.112 %\n"
     ]
    }
   ],
   "source": [
    "print(\"MEJOR F1 EN VALIDACIÓN\", round(best_f1_score*100, 3), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
